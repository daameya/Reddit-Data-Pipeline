id,title,selftext,score,num_comments,author,created_utc,url,upvote_ratio,over_18,edited,spoiler,stickied
1e8qpba,I passed the GCP Data Engineer Cert without prior professional exp on the platform,"A little bit info about my background, I have slightly more than 1 year of experience as Data Analyst and am currently on a career break trying to get my first DE position and decided to take the certification to open up more posibilities.

So yeah, the exam was really hard and extremely detailed, exceed my expectation even after 8 months of preparation but I'm super glad to pass the test. 

What's in my exam can be divided into these categories:

Core products for DE, you should have a firm grasp on the fundamentals of these, plus alternative solutions.   
e.g how to optimize, how to handle failure, how to operate with CLI commands, etc 

* GCS
* BigQuery
* Cloud Dataflow
* Cloud Composer
* CloudSQL, Cloud Spanner
* Cloud BigTable
* Dataproc (Cluster, Severless, Preemptible VM)
* Compute Engine
* IAM

Outside of that, some less widely-knowned products that appeared in my exam:

* Datastream
* BigQuery Omni
* Dataplex
* Data Fusion
* Analytics Hub
* Dataform

Thankfully, I read about some of them beforehand, shoutout to this [thread](https://www.reddit.com/r/dataengineering/comments/1csmm7v/just_cleared_the_gcp_professional_data_engineer/).

Anyway, I hope this information is useful to some of you who may want to take the test in the near future. Cheers!",59,13,EdenC13,2024-07-21 16:50:00,https://www.reddit.com/r/dataengineering/comments/1e8qpba/i_passed_the_gcp_data_engineer_cert_without_prior/,0,False,False,False,False
1e8ww0w,Why create a fact table surrogate key if the table has a proper primary key?,"Im reading the “bible”, Kimbell’s dimension modeling book.

The authors discuss a surrogate key for a facts table. If the table has a proper primary key, meaning autogenerated integer with no business value, then is there a need for a surrogate key? It seems redundant but I could be missing something.",48,16,Cultured_dude,2024-07-21 21:18:02,https://www.reddit.com/r/dataengineering/comments/1e8ww0w/why_create_a_fact_table_surrogate_key_if_the/,0,False,False,False,False
1e8hbnz,So many useless data pipelines are getting created…,"I feel like other than a couple of the important pipelines that calculate the core KPIs and business metrics, the rest are pretty useless. Don’t get me wrong, they are pretty to look at but IMO they do not serve any purpose.


I often get requests for tables/dashboards that sound like great idea initially but after implementation, the hype dies down and it goes into the pipeline graveyard that barely gets any eyes on it.

How do you decide which pipelines are important or not?

What are some examples of pipelines that you had to create that turned out to be useless?

What are the top used pipelines in your company?



",45,7,Bright_Bunch_7208,2024-07-21 08:03:14,https://www.reddit.com/r/dataengineering/comments/1e8hbnz/so_many_useless_data_pipelines_are_getting_created/,0,False,False,False,False
1e8n2gi,How micro is a Micro-Service,"My client has decided that every step of the Extact, Transform, and Load (ETL) process should be an individual Docker container. Usually I just have a single Docker container dedicated to sourcing data from external sources. And with the newest thought I would have to break apart and rework all previously constructed ETL - the definition of a changing scope. 

Before I go ahead and comply with the client's request I would like to discuss how micro is a micro-service - so that I don't have to redo work again. 

Any thoughts on Micro-Service Architectures and how to deal with clients that constantly change scope? ",21,21,harpooooooon,2024-07-21 14:05:05,https://www.reddit.com/r/dataengineering/comments/1e8n2gi/how_micro_is_a_microservice/,0,False,False,False,False
1e92jf1,Kimball Modelling - Article discussing the business process of model creation,"Happy to share the [second part](https://medium.com/@nydas/kimball-star-schemas-in-data-warehousing-part-2-4ee64be25cf3?source=friends_link&sk=94919b6ef2ce92b93fd06f4b869f22a2) of my three part article mini-series on Kimball modelling. The [first article](https://medium.com/@nydas/kimball-star-schemas-in-data-warehousing-part-1-0ed765574712?source=friends_link&sk=33dcb7916ce7845e46aa986fb56902f1) looked at dimension tables, while this second article dives into the business process of creating fact tables - the glue that holds your presentation layer together.

I hope this is useful to those of you starting out on your analytics engineering journey.

As always, my links to this sub are paywall bypassed.",10,0,nydasco,2024-07-22 01:47:13,https://www.reddit.com/r/dataengineering/comments/1e92jf1/kimball_modelling_article_discussing_the_business/,0,False,False,False,False
1e8irz1,Building Data Platforms from Scratch VS Third-Party Platforms,"Recently revisited [this informative post](https://medium.com/@hugolu87/open-source-vs-managed-data-architectures-which-one-should-you-choose-4a4eacee217a) on OSS vs managed data architectures, and it made me think: why do data teams/orgs decide to build things themselves vs using third party platforms?



To clarify I’m not comparing OSS vs managed - I’m comparing \*\*building solutions from scratch (i.e. OSS + managed) VS third-party platforms\*\*.



I have a semi-hot take which is: I think orgs should use third-party vendors (e.g. rivery, airbyte cloud, coalesce.io) because total cost of ownership will be lower. 



This is instead of hiring a number of platform/devops engineers to provision out EC2 instances, IaC, CICD pipelines, manage K8s clusters, Lambda functions, Python code, Kafka clusters etc.



However, I want to ask team leads, project managers, those with far more experience than me on big greenfield projects what their take is as I feel I could be missing something obvious e.g. if you’re ingesting 100TB/month, almost always in-house solutions from scratch  trumps third-party vendors?



Why I think data strategies should be made up of third-party vendors instead of built from scratch:

1. Quicker Development and Go-Live Times: No need to deal with infrastructure, which means the business can make data-informed decisions faster.

2. Less Maintenance Required: Infrastructure is managed by the vendor, allowing more time to be spent on development rather than maintenance.

3. Lower Labour Costs: Minimal data platform and DevOps expertise required, leading to reduced labour costs.

4. Potential for Discounts: Long-term contracts with vendors can often be negotiated for discounts.



Arguments Against:

1. Expense:

   - Platform and DevOps engineers who build and maintain the infrastructure and data products are also expensive.

2. Vendor Lock-In:

   - Some vendors provide backend code (e.g., y42).

   - In-house solutions also depend on the engineers who built them, with knowledge often spread among a few individuals, and documentation may be scarce or outdated.

3. Security:

   - orgs already use tools like Excel, Snowflake, Tableau, AWS, and Informatica. Is there a difference? (Genuinely asking).

4. Customization:

   - Vendors offer customizable options.

   - orgs can build specific use cases themselves rather than every single use case.

5. Service Downtime:

   - Vendors likely have quicker recovery times as downtime affects all their customers.

   - orgs should have a manual backup process in place, such as a standalone Python script to fetch raw data and upload it to S3, then rerun the pipeline.



TLDR I think using third-party vendors in the long run makes more sense. For those with greater insight and experience, please share your experiences - happy to be proven wrong and learn.",9,3,theoriginalmantooth,2024-07-21 09:51:15,https://www.reddit.com/r/dataengineering/comments/1e8irz1/building_data_platforms_from_scratch_vs/,0,False,False,False,False
1e8vw6k,How to use shared lake storage with diff computes (spark/fabric/databricks) to avoid vendor lock-ins ?,"Is the idea of a shared storage layer with the possibility of switching the compute engine real ? Say I decide to setup the data lake in Azure ADLS , and use databricks to process a set of data, and another set of pipelines are handled by fabric or some other oss engine. 

Does adopting unity catalog cause those databricks managed tables to become non usable by fabric? Is this where Iceberg Catalog comes to the rescue ?

What will be the cost elements to watch for? Will there be additional cost for transferring data between data storage and compute(dbrk/fabric), Or it is the standard cost in using compute engines ?

Sorry for lots of questions, I am just trying to understand data lake mgmt from a multi-vendor/OSS perspective. ",3,1,rasviz,2024-07-21 20:33:58,https://www.reddit.com/r/dataengineering/comments/1e8vw6k/how_to_use_shared_lake_storage_with_diff_computes/,0,False,False,False,False
1e96ee4,Are Tableau Prep flows pipelines?,"Before you hit me with the downvote, some context:

>We were reviewing candidates for a Sr. DE position, and one of them explicitly stated, on both their CV and screening  call, to be extremely proficient in designing, developing, and deploying data pipelines.  
On call, all their examples were based on Tableau Prep.. meaning they used a data connector, pulled the database either through drag and drop or a custom sql statement, then transformed it using standard steps, and published to Tablea Server. The flow was scheduled on the server as well, through UI.

Now my question is, are these really Pipelines by definition? ",3,6,samjenkins377,2024-07-22 05:23:55,https://www.reddit.com/r/dataengineering/comments/1e96ee4/are_tableau_prep_flows_pipelines/,1,False,False,False,False
1e8j9f0,Has anyone used the firestore to big query streaming extension. If then how can you partition the data? Is it possible to partition the data with a field available within the data field json?,#gcp,2,0,gooner4lifejoe,2024-07-21 10:26:21,https://www.reddit.com/r/dataengineering/comments/1e8j9f0/has_anyone_used_the_firestore_to_big_query/,0,False,False,False,False
1e97qc2,AI in data engineering,"Hi all,

With the revolution of recent AI technologies I wondered if and where you have used AI in regards of data engineering projects. I am not talking only about copilots for intelligent autocomplete etc. But more sophisticated usecases. Maybe productive ones ideally. For example in some of our projects we use different AI APIs to document our python code automatically and also to migrate old environments from one SQL dialect into another (thousands of script files required automated approach), including simplifying the code during migration and testing this code. 

Do you have any use cases where AI helped you working with data or data architecture?

",3,0,Dear_Boysenberry_521,2024-07-22 06:52:34,https://www.reddit.com/r/dataengineering/comments/1e97qc2/ai_in_data_engineering/,1,False,False,False,False
1e97o71, Best practices for handling replicated data from RDS to Redshift  for DBT workflows,"Hello,   
I'm currently working in a new project which uses Redshift as a DWH., and there is a DMS in the background that is replicating data from MySQL RDS to Redshift. 

I have a task to introduce DBT technology for Transformations on Redshift. And I have some questions/concerns.  
  
When replicating data from RDS to Redshift, I'm wondering about the best approach to handle this data in my dbt workflow. I'm considering two options and would appreciate your insights:

Treat replicated tables as raw/untouchable source data:

* Keep the replicated data unchanged
* Run DBT workflows which basically are coping (maybe some little changes) these tables to new tables in my target ""dbt"" schema - and at this stage add dist keys, sortkeys to copied table. Let's call it my staging layer - but with table materializations
* Create next layers based on these tables: intermediate/marts etc

Modify the raw replicated tables:

* Add distkey and sortkey directly to the raw replicated tables in Redshift
*  Still use dbt for further transformations and modeling, but in these casse treat ""replicated tables"" as my source data that is already prepared,and  modified for  staging layer.

So in the end I wonder if it's okey to touch replicated tables and build on top of them, or better to leave them untouched and just copy them in proper form into new base layer for my dbt workflows.

Which approach is generally considered best practice? Are there scenarios where one might be preferred over the other?

Additionally, if I go with the second approach, how can I automate the process of adding or updating distkey and sortkey for newly replicated tables in Redshift?

I'd appreciate any advice, especially from those who have experience with similar setups. Thanks in advance!",1,2,Purple_Wrap9596,2024-07-22 06:48:30,https://www.reddit.com/r/dataengineering/comments/1e97o71/best_practices_for_handling_replicated_data_from/,1,False,False,False,False
1e97i9d,"I want to bridge excel ""models"" with SQL, is it possible?","I'm tired of excel data manipulation and want to provide a better approach to this problem. 

My job uses excel models (around 20 different models), which is an excel file with ~10-15 worksheets, an input sheet and 2-5 output sheets, between them input variables populate the model that consists in sums, conditional logic between tables, ranges, constant values and different arithmetic operations. Between 100-500 input variables and around 400 output variables.

The ETL process is now automated ( set input data, calculate workbook and extract output ranges) but takes some time to process 3-10 min

This design tries to bring closer two groups of people, excel maintainers with no programming knowledge and a web application (my team) that through a web interface provides input variables and display output variables.

Do you know of a technique that converts excel models and its relationships into SQL or another function?

Do you know if low-code SQL is mature enough for this type of applications?",1,0,erlototo,2024-07-22 06:37:09,https://www.reddit.com/r/dataengineering/comments/1e97i9d/i_want_to_bridge_excel_models_with_sql_is_it/,1,False,False,False,False
1e8o0j1,"How AI, ML, Gen AI, and LLM Are Related","If you are confused about what is the relationship between AI, ML, Gen AI, and LLM. This article can help you.

[https://devblogit.com/understanding-artificial-intelligence-ai-hierarchy](https://devblogit.com/understanding-artificial-intelligence-ai-hierarchy)",0,2,Bubbly_Bed_4478,2024-07-21 14:50:16,https://www.reddit.com/r/dataengineering/comments/1e8o0j1/how_ai_ml_gen_ai_and_llm_are_related/,0,False,False,False,False
